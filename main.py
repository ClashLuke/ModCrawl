import deps
from random import randint

##====================================##
##  ##============================##  ##
##  ##                            ##  ##
##  ##        MAIL HARVEST        ##  ##
##  ##                            ##  ##
##  ##============================##  ##
##====================================##


def guide():
	main_url = deps.input_default("Please enter your root url.", "https://www.example.com/")
	deps.clear()
	proxy_source = deps.input_default("Please enter the name of your http(s)-proxy list.", "proxies.txt")
	deps.clear()
	while True:
		try:
			open(proxy_source)
			break
		except:
			print('Cannot find "' + proxy_source + '". Please make sure its in the same directory as this script')
			deps.clear()
	    
	out = deps.input_default("Now enter the name of the file you want your output to go to.", "out.txt")
	deps.clear()
	max_recursion = deps.input_default("How many Iterations should the recursion do?", 2, "int")
	deps.clear()
	regex = deps.input_default("What RegEx do you want to use?", "[a-z0-9.]+@[a-z0-9]+.[a-z]{1-3}")
	deps.clear()
	start(main_url, proxy_source, out, max_recursion, regex)


def start(main_url, proxy_source, out, max_recursion, regex):
	print("\033[1;33m[+]\t\033[0mPreperation Started!")
	#Creating important (temporary) textfiles
	t = open("0.txt", "w").close()
	t = open("1.txt", "w").close()
	t = open(out, "w").close()
	proxy_list = open(proxy_source, "r").read().replace('\r','\n').replace('\n\n','\n').split('\n')
	urls = [main_url]
	print("\033[1;33m[+]\033[0m Getting URLs from first Root URL")
	deps.req(main_url, proxy_list[0], "1.txt", r"((http:\/\/|https:\/\/)[a-zA-Z0-9]*([\-\.]{1}[a-zA-Z0-9]+)*\.([a-zA-Z]*[0-9]*(\/)?(\.)?(\%)?(\&)?(\")?(\\A7)?(\;)?(\:)?(\+)?(\?)?(\=)?)*)")
	deps.req(main_url, proxy_list[0], out, regex)
    	#Entering Recursion
	print("\033[1;33m[-]\t\033[0mEntering Loop that checks for URLs and Results..")
	for i in range(1, 1+max_recursion):
		p_list = proxy_list
		print("\r\n\r\n\r\n\033[1;33m[+]\t\033[0mStarting " + str(i) + ". Iteration")
		print("\033[1;31m[-]\tRemoving duplicates")
		print("\t\t\033[1;31m[-]\033[0mRemoving in URL List, Status:\t" + str(deps.check_duplicates("0.txt","1.txt")))
		print("\t\t\033[1;31m[-]\033[0mRemoving in Result List, Status:\t" + str(deps.check_duplicates(out)))
		print("\t\t\033[1;31m[-]\033[0mMerging lists of checked results, Status:\t" + str(deps.merge("0.txt", "1.txt", "0.txt")))
		urls = open("1.txt", "r").read().replace("\r","\n").replace("\n\n","\n").split("\n")
		mails = open(out, "r").read().replace("\r","\n").replace("\n\n","\n").split("\n")
		if urls == []:
			break
		print("\033[1;33m[-]\t\033[0mFound " + str(len(urls)) + " URLs generated by the previous generation and")
		print("\033[1;33m[-]\t\033[0mFound " + str(len(mails)) + " Results found by previous generations")
		print("\033[1;33m[-]\t\033[0mChecking for URLs and Results on previously found sites")
		for num in range(len(urls)):
			ran = randint(0, len(p_list)-1)
			p = p_list[ran]
			p_list[ran]
			deps.req(urls[num], p, "1.txt", r"((http:\/\/|https:\/\/)[a-zA-Z0-9]*([\-\.]{1}[a-zA-Z0-9]+)*\.([a-zA-Z]*[0-9]*(\/)?(\.)?(\%)?(\&)?(\")?(\\A7)?(\;)?(\:)?(\+)?(\?)?(\=)?)*)")
			deps.req(urls[num], p, out, regex)
			print("\033[1;33m[-]\t\033[0mRemoving old files")
     

##====================================##
##  ##============================##  ##
##  ##                            ##  ##
##  ##          START UP          ##  ##
##  ##                            ##  ##
##  ##============================##  ##
##====================================##


if __name__ == "__main__":
    guide()
